{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тестовое множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение\n",
    "\n",
    "### 1. Обработать данные, получить токены, обработать токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='овечкин', tag=OpencorporaTag('NOUN,anim,masc,Sgtm,Surn sing,nomn'), normal_form='овечкин', score=1.0, methods_stack=((DictionaryAnalyzer(), 'овечкин', 37, 0),))]\n",
      "[Parse(word='пожертвовал', tag=OpencorporaTag('VERB,perf,tran masc,sing,past,indc'), normal_form='пожертвовать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'пожертвовал', 748, 1),))]\n",
      "[Parse(word='детской', tag=OpencorporaTag('ADJF femn,sing,gent'), normal_form='детский', score=0.394736, methods_stack=((DictionaryAnalyzer(), 'детской', 16, 8),)), Parse(word='детской', tag=OpencorporaTag('ADJF femn,sing,loct'), normal_form='детский', score=0.263157, methods_stack=((DictionaryAnalyzer(), 'детской', 16, 13),)), Parse(word='детской', tag=OpencorporaTag('ADJF femn,sing,ablt'), normal_form='детский', score=0.131578, methods_stack=((DictionaryAnalyzer(), 'детской', 16, 11),)), Parse(word='детской', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='детская', score=0.078947, methods_stack=((DictionaryAnalyzer(), 'детской', 144, 1),)), Parse(word='детской', tag=OpencorporaTag('ADJF femn,sing,datv'), normal_form='детский', score=0.052631, methods_stack=((DictionaryAnalyzer(), 'детской', 16, 9),)), Parse(word='детской', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='детская', score=0.026315, methods_stack=((DictionaryAnalyzer(), 'детской', 144, 2),)), Parse(word='детской', tag=OpencorporaTag('NOUN,inan,femn sing,ablt'), normal_form='детская', score=0.026315, methods_stack=((DictionaryAnalyzer(), 'детской', 144, 4),)), Parse(word='детской', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='детская', score=0.026315, methods_stack=((DictionaryAnalyzer(), 'детской', 144, 6),))]\n",
      "[Parse(word='хоккейной', tag=OpencorporaTag('ADJF,Qual femn,sing,gent'), normal_form='хоккейный', score=0.636363, methods_stack=((DictionaryAnalyzer(), 'хоккейной', 97, 8),)), Parse(word='хоккейной', tag=OpencorporaTag('ADJF,Qual femn,sing,loct'), normal_form='хоккейный', score=0.181818, methods_stack=((DictionaryAnalyzer(), 'хоккейной', 97, 13),)), Parse(word='хоккейной', tag=OpencorporaTag('ADJF,Qual femn,sing,datv'), normal_form='хоккейный', score=0.090909, methods_stack=((DictionaryAnalyzer(), 'хоккейной', 97, 9),)), Parse(word='хоккейной', tag=OpencorporaTag('ADJF,Qual femn,sing,ablt'), normal_form='хоккейный', score=0.090909, methods_stack=((DictionaryAnalyzer(), 'хоккейной', 97, 11),))]\n",
      "[Parse(word='школе', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='школа', score=0.875, methods_stack=((DictionaryAnalyzer(), 'школе', 55, 6),)), Parse(word='школе', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='школа', score=0.125, methods_stack=((DictionaryAnalyzer(), 'школе', 55, 2),))]\n",
      "[Parse(word='автомобиль', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='автомобиль', score=0.571428, methods_stack=((DictionaryAnalyzer(), 'автомобиль', 92, 0),)), Parse(word='автомобиль', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='автомобиль', score=0.428571, methods_stack=((DictionaryAnalyzer(), 'автомобиль', 92, 3),))]\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "data = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Read Text\n",
    "with open(\"data/news_train.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        vec = []\n",
    "        category, topic, content = line.split(\"\\t\")\n",
    "        vec.append(category)\n",
    "        vec.append(topic)\n",
    "        vec.append(content)\n",
    "        data.append(vec)\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "data_tokens = []\n",
    "\n",
    "for vec in data:\n",
    "    vec_tokenize = [vec[0]]\n",
    "    sentences = sent_tokenize(vec[1])\n",
    "    vec_tokenize.append([word_tokenize(sent) for sent in sentences])\n",
    "    sentences = sent_tokenize(vec[1])\n",
    "    vec_tokenize.append([word_tokenize(sent) for sent in sentences])\n",
    "    data_tokens.append(vec_tokenize)\n",
    "\n",
    "# Process tokens\n",
    "\n",
    "counter = 0\n",
    "token_limit = 5\n",
    "\n",
    "\n",
    "for vec in data_tokens:\n",
    "    for i in range(1,2):\n",
    "        sentence = vec[i]\n",
    "        for tokens in sentence:\n",
    "            for token in tokens:\n",
    "                print(morph.parse(token))\n",
    "                counter += 1\n",
    "                if counter > token_limit:\n",
    "                    break\n",
    "            if counter > token_limit:\n",
    "                break\n",
    "        if counter > token_limit:\n",
    "            break\n",
    "    if counter > token_limit:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Применить word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 8431\n",
      "[('украина', 0.9993064999580383), ('бывший', 0.9992741346359253), ('российский', 0.9992659091949463), ('умереть', 0.9992592334747314), ('клуб', 0.9992372393608093), ('сша', 0.9992058277130127), ('найти', 0.9991931915283203), ('представить', 0.999170184135437), ('оказаться', 0.9991665482521057), ('интернет', 0.9991496205329895)]\n"
     ]
    }
   ],
   "source": [
    "# Fill all words for train\n",
    "\n",
    "words_arr = []\n",
    "\n",
    "for vec in data_tokens:\n",
    "    words = []\n",
    "    for sentence in vec[2]:\n",
    "        for word in sentence:\n",
    "            token = morph.parse(word)[0].normal_form  # Take the first instance\n",
    "            if token not in stopwords.words(\"russian\"):\n",
    "                words.append(token)\n",
    "    words_arr.append(words)\n",
    "\n",
    "word2vec = Word2Vec(words_arr, min_count=2,vector_size=50)\n",
    "vocabulary = word2vec.wv.index_to_key\n",
    "print(\"Размер словаря:\",len(vocabulary))\n",
    "\n",
    "sim_words = word2vec.wv.most_similar(\"россия\")\n",
    "print(sim_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Векторизация предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "v1 = word2vec.wv[\"россия\"]\n",
    "print(v1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41860983  0.29179952  0.07225689 -0.07121758 -0.31129956 -1.1069416\n",
      "  1.4046897   2.1987472  -1.5065944  -0.42231905 -0.3255589  -1.0652158\n",
      " -0.21291277  0.6928413  -1.0033861   0.05026566  0.8463753  -0.35104603\n",
      " -1.6473411  -1.2871469   0.48821086  1.1424234   2.1425054  -1.1257832\n",
      "  0.8041436   0.4309289  -0.8419055   0.37538263 -1.5419481   0.6640646\n",
      " -0.07586268  0.24870017 -0.30461776  0.42159936 -0.9463336   1.2704792\n",
      "  0.6442732   0.26367396  0.45819473 -0.34234995  0.8988328  -0.29495433\n",
      " -0.7231193   0.11126491  2.540054    0.07222302  0.16004148 -1.3178422\n",
      "  1.0574048   1.0542439 ]\n"
     ]
    }
   ],
   "source": [
    "sentences_vec = []\n",
    "\n",
    "for sentence in words_arr:\n",
    "    try:\n",
    "        vec = word2vec.wv[sentence[0]]\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(1,len(sentence)):\n",
    "        try:\n",
    "            vec = vec + word2vec.wv[sentence[i]]\n",
    "        except:\n",
    "            pass\n",
    "    sentences_vec.append(vec)\n",
    "\n",
    "print(sentences_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[0, 1, 2, 0, 3, 0, 4, 1, 4, 4]\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "y_uniq = []\n",
    "y = []\n",
    "for vec in data_tokens:\n",
    "    category = vec[0]\n",
    "    if y_uniq.count(category) == 0:\n",
    "        y_uniq.append(category)\n",
    "    y.append(y_uniq.index(category))\n",
    "\n",
    "print(len(y_uniq))\n",
    "print(y[:10])\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настроим SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovr',probability=True)\n",
    "clf.fit(sentences_vec, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим тестовое множество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = []\n",
    "\n",
    "# Read Text\n",
    "with open(\"data/news_test.txt\", 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        vec = []\n",
    "        category, topic, content = line.split(\"\\t\")\n",
    "        vec.append(category)\n",
    "        vec.append(topic)\n",
    "        vec.append(content)\n",
    "        data_test.append(vec)\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "\n",
    "data_tokens_test = []\n",
    "\n",
    "\n",
    "for vec in data_test:\n",
    "    vec_tokenize = [vec[0]]\n",
    "    sentences = sent_tokenize(vec[1])\n",
    "    vec_tokenize.append([word_tokenize(sent) for sent in sentences])\n",
    "    sentences = sent_tokenize(vec[1])\n",
    "    vec_tokenize.append([word_tokenize(sent) for sent in sentences])\n",
    "    data_tokens_test.append(vec_tokenize)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "words_arr_test = []\n",
    "\n",
    "for vec in data_tokens_test:\n",
    "    words = []\n",
    "    for sentence in vec[2]:\n",
    "        for word in sentence:\n",
    "            token = morph.parse(word)[0].normal_form  # Take the first instance\n",
    "            if token not in stopwords.words(\"russian\"):\n",
    "                words.append(token)\n",
    "    words_arr_test.append(words)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посчитаем вероятности по категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10)\n",
      "[0.24419811 0.21459631 0.06286252 0.12742638 0.11750383 0.04703241\n",
      " 0.09037877 0.01474793 0.02948599 0.05176775]\n"
     ]
    }
   ],
   "source": [
    "sentences_vec_test = []\n",
    "\n",
    "for sentence in words_arr_test:\n",
    "    try:\n",
    "        vec = word2vec.wv[sentence[0]]\n",
    "    except:\n",
    "        pass\n",
    "    for i in range(1,len(sentence)):\n",
    "        try:\n",
    "            vec = vec + word2vec.wv[sentence[i]]\n",
    "        except:\n",
    "            pass\n",
    "    sentences_vec_test.append(vec)\n",
    "\n",
    "dec = clf.predict_proba(sentences_vec_test)\n",
    "\n",
    "print(dec.shape)\n",
    "print(dec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим процент попаданий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838 / 3000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "\n",
    "for i in range(len(dec)):\n",
    "    decision = dec[i]\n",
    "    index = np.where(decision == max(decision))[0][0]\n",
    "    if index == y_uniq.index(data_tokens_test[i][0]):\n",
    "        counter+=1\n",
    "        \n",
    "print(counter, \"/\", len(dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полимеальное ядро уменьшало процент попаданий. А увеличение размеронсти вектора до 100 или 500, никак не влияло."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем Наивный Байевский Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalNB()"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = CategoricalNB()\n",
    "\n",
    "gnb.fit(NormalizeData(sentences_vec)  , y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предскажем результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = gnb.predict(NormalizeData(sentences_vec_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 / 3000\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "\n",
    "for i in range(len(dec)):\n",
    "    decision = dec[i]\n",
    "    if decision == y_uniq.index(data_tokens_test[i][0]):\n",
    "        counter+=1\n",
    "        \n",
    "print(counter, \"/\", len(dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
